{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8Yrl9IbF_dv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rng = torch.Generator().manual_seed(137)"
      ],
      "metadata": {
        "id": "cZTf2qz3VRr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start today's tutorial with the MNIST dataset, which is widely used in the deep learning.\n",
        "This is labeled black-white images of numbers from 0 to 9."
      ],
      "metadata": {
        "id": "MGZpCmX56kb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor() # Let's assume that the all data is in PyTorch Tensor format\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Get the data and labels as tensors\n",
        "x_train, y_train = train_dataset.data, train_dataset.targets\n",
        "x_test, y_test = test_dataset.data, test_dataset.targets\n",
        "\n",
        "fig,ax=plt.subplots(ncols=6)\n",
        "for i,a in zip(range(6),ax):\n",
        "  a.imshow(x_train[i], cmap='gray')\n",
        "  a.set_title(y_train[i].numpy())\n",
        "  a.axis('off')"
      ],
      "metadata": {
        "id": "NifTc4tp6_Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0].shape)"
      ],
      "metadata": {
        "id": "Jr4ZODxo74oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Sequential Architecture-Building\n",
        "Let's start from the fully-connected (FC) deep neural network. Probably the easiest way to build it is to use the sequential method."
      ],
      "metadata": {
        "id": "-qfexBnaXzsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start model definition by using Sequential()\n",
        "model_fc = torch.nn.Sequential()\n",
        "\n",
        "# From now on, let's build it by adding each layer or function one by one.\n",
        "# To add a layer, we use add_module()\n",
        "model_fc.add_module('flatten',torch.nn.Flatten()) # Make the 2D image into 1D array\n",
        "model_fc.add_module('fc1',torch.nn.Linear(28*28,128)) # Fully-connected hidden layer 1 with 128 nodes\n",
        "model_fc.add_module('relu1',torch.nn.ReLU()) # Add nonlinearity by normalizing it between 0-1\n",
        "model_fc.add_module('fc2',torch.nn.Linear(128,128)) # Fully-connected hidden lyaer 2 with 128 nodes\n",
        "model_fc.add_module('relu2',torch.nn.ReLU()) # Same as before\n",
        "model_fc.add_module('fc3',torch.nn.Linear(128,10)) # Output layer with 10 node (0-9)"
      ],
      "metadata": {
        "id": "5MD4iAGGaKmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the model to GPU if possible\n",
        "# You need to change your runtime environment at the upper right menu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_fc.to(device)\n",
        "print(device)\n",
        "\n",
        "# Let's print the summary of the model\n",
        "import torchsummary\n",
        "torchsummary.summary(model_fc,(28,28),device=str(device))"
      ],
      "metadata": {
        "id": "Mh0gNcGhlpLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each hidden layer, we applied sigmoid function as an **activation function**.\n",
        "Activation function is a function used for adding **nonlinearity** to the process.\n",
        "\n",
        "There could be many different choices of activation functions, including your own custom functions.\n",
        "Some famous activation functions are shown below.\n",
        "While ReLU (rectified linear unit) and its deviations are famous nowadays, there is no global strict rule for selecting activation functions.\n",
        "However, you need to be careful for the activation function at the output layer, as different activation functions give different target set.\n",
        "![Activation Function](https://cdn-images-1.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png)\n",
        "\n",
        "(https://cdn-images-1.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png)"
      ],
      "metadata": {
        "id": "sK6-T0dZmws9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compile the model.\n",
        "\n",
        "One of the important thing is the definition of **loss function**, the function that we want to minimize.\n",
        "If the output is the continous variables, then the loss function can be the **mean square error**,\n",
        "$$ \\mathcal{L}_{\\rm mse} = \\sum_i (y_i^{\\rm true} - y_i^{\\rm predict})^2 \\, ,$$\n",
        "which is similar to the ordinary $\\chi^2$-fitting.\n",
        "\n",
        "On the other hand, if the output is the categorical probability, then the loss function can be **categorical cross-entropy**,\n",
        "$$ \\mathcal{L}_{\\rm cce} = -\\sum_i p_i^{\\rm true} \\log_{10} p_i^{\\rm predict} \\, .$$\n",
        "\n",
        "Note that one can still select different kinds of loss function, depending on your problem.\n",
        "\n",
        "Another important thing is the learning rate, which determines how fast the weights will be changed during the learning process.\n",
        "If the learning rate is too low, it takes too much time to reach the minimum of the loss function and stuck in the nearest local minima rather than the global minimum.\n",
        "On the other hand, if the learning rate is too high, it can easily escape the minimum.\n",
        "![Choice of Learning Rate](https://pyimagesearch.com/wp-content/uploads/2019/08/keras_learning_rate_finder_header.png)\n",
        "\n",
        "(https://pyimagesearch.com/wp-content/uploads/2019/08/keras_learning_rate_finder_header.png)"
      ],
      "metadata": {
        "id": "iA4d-5fEnEDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model_fc.parameters(),lr=1e-2) # There are numerous optimizer options, but I used a simple one\n",
        "criterion = torch.nn.CrossEntropyLoss() # This loss function is designed for classification"
      ],
      "metadata": {
        "id": "SvZ4-9RQnTuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's perform the actual fit!\n",
        "\n",
        "You can decide the number of epochs (a cycle that you perform the training and validation), the number of minibatches (a number of minimal training patches you use for a single fitting), and the split ratio between training and validation samples."
      ],
      "metadata": {
        "id": "Rvlj7suEpKUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm # for adding the visual progress bar\n",
        "\n",
        "# Split training dataset further --- training & validation\n",
        "len_train = int(0.8*len(train_dataset))\n",
        "len_valid = len(train_dataset) - len_train\n",
        "trainset, validset = torch.utils.data.random_split(\n",
        "    train_dataset,\n",
        "     [len_train, len_valid],\n",
        "    generator=rng\n",
        ")\n",
        "\n",
        "# Define the number of epochs and batch size\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "# Create DataLoaders for training and validation sets\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "validloader = torch.utils.data.DataLoader(validset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Lists to store loss values for plotting\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    model_fc.train() # Set the model to training mode\n",
        "    for data in tqdm(trainloader):\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Move data to the same device as the model\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_fc(inputs)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    epoch_train_loss = running_loss / len(trainloader)\n",
        "    train_loss_history.append(epoch_train_loss)\n",
        "\n",
        "    # Validation step\n",
        "    model_fc.eval() # Set the model to evaluation mode\n",
        "    valid_loss = 0.0\n",
        "    with torch.no_grad(): # Disable gradient calculation for validation\n",
        "        for data in validloader:\n",
        "            inputs, labels = data\n",
        "\n",
        "            # Move data to the same device as the model\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model_fc(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "    # Calculate average validation loss for the epoch\n",
        "    epoch_valid_loss = valid_loss / len(validloader)\n",
        "    valid_loss_history.append(epoch_valid_loss)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Training Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_valid_loss:.4f}')\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "TOdWYoqCCXq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before applying the trained model to the samples, let's check if the training is done well.\n",
        "One way is to check the evolution of loss function per epoch, both for train and validation samples."
      ],
      "metadata": {
        "id": "OtGiHBBHEbXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot the evolution of train and validation losses over epoch\n",
        "arrEpoch = np.arange(1,epochs+1)\n",
        "plt.plot(arrEpoch,train_loss_history,'r-',label=\"Train Loss\")\n",
        "plt.plot(arrEpoch,valid_loss_history,'b-',label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss (Categorical Cross-Entropy)\")\n",
        "plt.yscale(\"log\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "x5Ph2-tsD-zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If both the train and validation losses continue to decrease, then it may mean that there is a room for improvement if you keep the learning process more.\n",
        "\n",
        "On the other hand, if the train loss keeps decreasing and the validation loss starts to increase, then probably the learning process is in **overfitting** --- that is, the model eventually just copy the train sample without maintaining the general nature.\n",
        "You probably need to save the model before it happens and stop the learning.   "
      ],
      "metadata": {
        "id": "5B95XO7NEgEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate the accuracy of our new model by applying them to the test samples."
      ],
      "metadata": {
        "id": "wa9R2Dm1ElQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataLoader for the test set\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model_fc.eval() # Set the model to evaluation mode\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad(): # Disable gradient calculation for evaluation\n",
        "    for data in testloader:\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Move data to the same device as the model\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model_fc(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Get predictions\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_samples += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate average test loss and accuracy\n",
        "avg_test_loss = test_loss / len(testloader)\n",
        "accuracy = correct_predictions / total_samples\n",
        "\n",
        "print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "m_HElSNRFBlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's predict the test samples by using our new model, and compare it to the actual values."
      ],
      "metadata": {
        "id": "7o6tufVNFWFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_fc.eval() # Set the model to evaluation mode\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    # Get predictions for the first few test samples\n",
        "    outputs = model_fc(x_test[:4].float().to(device))\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "fig,ax=plt.subplots(ncols=4)\n",
        "for i,a in zip(range(4),ax):\n",
        "  a.imshow(x_test[i], cmap='gray')\n",
        "  a.set_title('Pred: {} Real: {}'.format(predicted.cpu()[i].numpy(),y_test[i].numpy()))\n",
        "  a.axis('off')"
      ],
      "metadata": {
        "id": "L3-TCDPuFw7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. More general method\n",
        "\n",
        "The sequantial method is the easiest way to build the deep learning architectures.\n",
        "However, as we have seen, one can only build the \"sequential\" architectures:\n",
        "$$ A \\to B \\to C \\to \\cdots \\to E \\, .$$\n",
        "\n",
        "However, in some cases you would like to build more complex architectures, such as\n",
        "$$ A \\to \\left\\{ \\begin{array}{c} B \\to \\cdots \\to C \\\\ D \\to \\cdots \\to E \\end{array} \\right\\} \\to F \\, .$$\n",
        "To build such complex architectures, one needs a full control of inputs and outputs of each layer.\n",
        "\n",
        "Let's rebuild the fully-connected architecture that we used in the previous section with a more general method."
      ],
      "metadata": {
        "id": "UY32vfv08YBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a class that inherits from nn.Module\n",
        "class SimpleNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Input layer: Flatten the image. The input size will be 28x28.\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.fc1 = torch.nn.Linear(28*28, 128) # Fully-connected hidden layer 1 with 128 nodes\n",
        "        self.fc2 = torch.nn.Linear(128, 128) # Fully-connected hidden layer 2 with 128 nodes\n",
        "        self.fc3 = torch.nn.Linear(128, 10) # Output layer with 10 nodes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input image\n",
        "        x = self.flatten(x)\n",
        "        # Apply the layers and activation functions\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model_fc_new = SimpleNN().to(device)"
      ],
      "metadata": {
        "id": "EAXwo5BTIIEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's print the summary of the model\n",
        "torchsummary.summary(model_fc_new,(28,28),device=str(device))"
      ],
      "metadata": {
        "id": "i8gj38GgJRcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Homework: Galaxy morphology classification\n",
        "\n",
        "We just learned a basic of deep learning by using the MNIST dataset. This dataset has been tested by tons of different machine learning methods for a long time. Probably that's why your deep learning architecture works well.\n",
        "\n",
        "But... will your model work well with other problems as well? Let's try it with galaxy image dataset with morphology (spiral or elliptical).\n",
        "\n"
      ],
      "metadata": {
        "id": "sLfKT9NFoccI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's download the data."
      ],
      "metadata": {
        "id": "Ctr03jN7psYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "-wyEBTaWGEzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1QjTECPvkzrXUJxishXS-0KTknNq0erly"
      ],
      "metadata": {
        "id": "W6eGQV5LGfla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q gz_data.zip"
      ],
      "metadata": {
        "id": "X1jPVSZPG_Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at example images:"
      ],
      "metadata": {
        "id": "C3KDB7PNUrxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "import glob"
      ],
      "metadata": {
        "id": "IA4VO23HToUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(ncols=2)\n",
        "for s, a in zip(['elliptical', 'spiral'], ax) :\n",
        "  a.imshow(PIL.Image.open(glob.glob(f'curated_data/{s}/*.jpg')[0]))\n",
        "  a.set_title(s)\n",
        "  a.axis('off')"
      ],
      "metadata": {
        "id": "nQY-vozzTvq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original RGB image has a size of 424 x 424 pixels. This might be too large for the tutorial. Let's reduce the image size so that we can train our deep learning models in a reasonable time."
      ],
      "metadata": {
        "id": "oXg3nmvRy3Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((27,27)),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "dataset = torchvision.datasets.ImageFolder('curated_data', transform=transform)"
      ],
      "metadata": {
        "id": "LgkrSGt1LwH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's split our data set into training, validation, and test data.\n",
        "\n",
        "It's very important to do things reproducibly!"
      ],
      "metadata": {
        "id": "DoushIu_U4Kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len_train = int(0.7*len(dataset))\n",
        "len_valid = int(0.2*len(dataset))\n",
        "len_test = len(dataset) - len_train - len_valid\n",
        "trainset, validset, testset = torch.utils.data.random_split(\n",
        "    dataset,\n",
        "     [len_train, len_valid, len_test],\n",
        "    generator=rng\n",
        ")"
      ],
      "metadata": {
        "id": "2r11g_x9L3qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Data set sizes:\\n\\ttraining:\\t{len(trainset)}\\n\\tvalidation:\\t{len(validset)}\\n\\ttesting:\\t{len(testset)}')"
      ],
      "metadata": {
        "id": "tS5ZSLkNSZ7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax=plt.subplots(ncols=6)\n",
        "for i,a in zip(range(6),ax):\n",
        "  a.imshow(np.transpose(trainset[i][0],(1,2,0)))\n",
        "  a.set_title(trainset[i][1])\n",
        "  a.axis('off')"
      ],
      "metadata": {
        "id": "Vm4yRZohXIyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "model_morph = ...\n",
        "\n",
        "model_morph.to(device)"
      ],
      "metadata": {
        "id": "za6TSK1x2JuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_morph = ...\n",
        "criterion_morph = ..."
      ],
      "metadata": {
        "id": "ZveXOdSq2VS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders for training and validation sets\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "validloader = torch.utils.data.DataLoader(validset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Lists to store loss values for plotting\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "epochs = ...\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    ...\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Training Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_valid_loss:.4f}')\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "07I1s2wmr7L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataLoader for the test set\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model_morph.eval() # Set the model to evaluation mode\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad(): # Disable gradient calculation for evaluation\n",
        "    for data in testloader:\n",
        "        ...\n",
        "\n",
        "# Calculate average test loss and accuracy\n",
        "avg_test_loss = test_loss / len(testloader)\n",
        "accuracy = correct_predictions / total_samples\n",
        "\n",
        "print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "WvWKB_PCs4Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_morph.eval() # Set the model to evaluation mode\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    # Get predictions for the first few test samples\n",
        "    inputs = torch.from_numpy(np.array([testset[i][0] for i in range(4)]))\n",
        "    outputs = model_morph(inputs.to(device))\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "fig,ax=plt.subplots(ncols=4)\n",
        "for i,a in zip(range(4),ax):\n",
        "  a.imshow(np.transpose(testset[i][0],(1,2,0)))\n",
        "  a.set_title('Pred: {} Real: {}'.format(predicted.cpu()[i],testset[i][1]))\n",
        "  a.axis('off')"
      ],
      "metadata": {
        "id": "7m9vyebyztwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}